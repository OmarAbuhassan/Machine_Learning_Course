{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees and Ensembles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forest Cover Prediction\n",
    "In this assignment we are going to predict the forest cover type (the predominant kind of tree cover) from strictly cartographic variables (as opposed to remotely sensed data). Cover_Type (7 types, integer 1 to 7). The seven types are:\n",
    "1. Spruce/Fir\n",
    "2. Lodgepole Pine\n",
    "3. Ponderosa Pine\n",
    "4. Cottonwood/Willow\n",
    "5. Aspen\n",
    "6. Douglas-fir\n",
    "7. Krummholz\n",
    "\n",
    "\"Predicting forest cover type from cartographic variables only (no remotely sensed data). The actual forest cover type for a given observation (30 x 30 meter cell) was determined from US Forest Service (USFS) Region 2 Resource Information System (RIS) data. Independent variables were derived from data originally obtained from US Geological Survey (USGS) and USFS data. Data is in raw form (not scaled) and contains binary (0 or 1) columns of data for qualitative independent variables (wilderness areas and soil types).\" [https://archive.ics.uci.edu/ml/datasets/covertype] \n",
    "\n",
    "In order to classify the forest cover, we will use several different classifiers and compare their results. The classifiers we will use are Decision Trees, Bagging, Boosting, and Random Forest. In this assignment you are suppose to use built-in classifiers from `sklearn`. The training, validation, and test partitions are provided. You may need to do some preprocessing, and of course hyper-parameter tuning for each classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "covtype = datasets.fetch_covtype()\n",
    "X = covtype.data\n",
    "Y = covtype.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((581012, 54), (581012,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "perm = np.random.permutation(581012)\n",
    "trainx = X[perm[0:49500],:]\n",
    "trainy = Y[perm[0:49500]]\n",
    "valx = X[perm[49500:55000],:]\n",
    "valy = Y[perm[49500:55000]]\n",
    "testx = X[perm[55000:581012],:]\n",
    "testy = Y[perm[55000:581012]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17945, 24251, 3023, 254, 786, 1481, 1760)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(trainy==1), sum(trainy==2), sum(trainy==3), sum(trainy==4), sum(trainy==5), sum(trainy==6), sum(trainy==7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]\n",
      "Setting criterion=gini, splitter=best, min_samples_split=2, train_acc=1.00, val_acc=0.84\n",
      "Setting criterion=gini, splitter=best, min_samples_split=3, train_acc=0.99, val_acc=0.84\n",
      "Setting criterion=gini, splitter=best, min_samples_split=4, train_acc=0.99, val_acc=0.84\n",
      "Setting criterion=gini, splitter=best, min_samples_split=5, train_acc=0.98, val_acc=0.84\n",
      "Setting criterion=gini, splitter=best, min_samples_split=6, train_acc=0.98, val_acc=0.83\n",
      "Setting criterion=gini, splitter=best, min_samples_split=7, train_acc=0.97, val_acc=0.83\n",
      "Setting criterion=gini, splitter=best, min_samples_split=8, train_acc=0.97, val_acc=0.83\n",
      "Setting criterion=gini, splitter=best, min_samples_split=9, train_acc=0.96, val_acc=0.83\n",
      "Setting criterion=gini, splitter=best, min_samples_split=10, train_acc=0.96, val_acc=0.82\n",
      "Setting criterion=gini, splitter=best, min_samples_split=11, train_acc=0.95, val_acc=0.82\n",
      "Setting criterion=gini, splitter=best, min_samples_split=12, train_acc=0.95, val_acc=0.82\n",
      "Setting criterion=gini, splitter=best, min_samples_split=13, train_acc=0.94, val_acc=0.82\n",
      "Setting criterion=gini, splitter=best, min_samples_split=14, train_acc=0.94, val_acc=0.82\n",
      "Setting criterion=gini, splitter=best, min_samples_split=15, train_acc=0.94, val_acc=0.82\n",
      "Setting criterion=gini, splitter=best, min_samples_split=16, train_acc=0.93, val_acc=0.82\n",
      "Setting criterion=gini, splitter=best, min_samples_split=17, train_acc=0.93, val_acc=0.82\n",
      "Setting criterion=gini, splitter=best, min_samples_split=18, train_acc=0.93, val_acc=0.82\n",
      "Setting criterion=gini, splitter=best, min_samples_split=19, train_acc=0.92, val_acc=0.81\n",
      "Setting criterion=gini, splitter=random, min_samples_split=2, train_acc=1.00, val_acc=0.83\n",
      "Setting criterion=gini, splitter=random, min_samples_split=3, train_acc=0.99, val_acc=0.82\n",
      "Setting criterion=gini, splitter=random, min_samples_split=4, train_acc=0.97, val_acc=0.83\n",
      "Setting criterion=gini, splitter=random, min_samples_split=5, train_acc=0.96, val_acc=0.82\n",
      "Setting criterion=gini, splitter=random, min_samples_split=6, train_acc=0.95, val_acc=0.82\n",
      "Setting criterion=gini, splitter=random, min_samples_split=7, train_acc=0.94, val_acc=0.82\n",
      "Setting criterion=gini, splitter=random, min_samples_split=8, train_acc=0.94, val_acc=0.82\n",
      "Setting criterion=gini, splitter=random, min_samples_split=9, train_acc=0.93, val_acc=0.82\n",
      "Setting criterion=gini, splitter=random, min_samples_split=10, train_acc=0.92, val_acc=0.81\n",
      "Setting criterion=gini, splitter=random, min_samples_split=11, train_acc=0.91, val_acc=0.82\n",
      "Setting criterion=gini, splitter=random, min_samples_split=12, train_acc=0.91, val_acc=0.81\n",
      "Setting criterion=gini, splitter=random, min_samples_split=13, train_acc=0.90, val_acc=0.81\n",
      "Setting criterion=gini, splitter=random, min_samples_split=14, train_acc=0.90, val_acc=0.81\n",
      "Setting criterion=gini, splitter=random, min_samples_split=15, train_acc=0.89, val_acc=0.81\n",
      "Setting criterion=gini, splitter=random, min_samples_split=16, train_acc=0.89, val_acc=0.80\n",
      "Setting criterion=gini, splitter=random, min_samples_split=17, train_acc=0.89, val_acc=0.81\n",
      "Setting criterion=gini, splitter=random, min_samples_split=18, train_acc=0.89, val_acc=0.81\n",
      "Setting criterion=gini, splitter=random, min_samples_split=19, train_acc=0.88, val_acc=0.81\n",
      "Setting criterion=entropy, splitter=best, min_samples_split=2, train_acc=1.00, val_acc=0.84\n",
      "Setting criterion=entropy, splitter=best, min_samples_split=3, train_acc=1.00, val_acc=0.84\n",
      "Setting criterion=entropy, splitter=best, min_samples_split=4, train_acc=0.99, val_acc=0.84\n",
      "Setting criterion=entropy, splitter=best, min_samples_split=5, train_acc=0.99, val_acc=0.83\n",
      "Setting criterion=entropy, splitter=best, min_samples_split=6, train_acc=0.98, val_acc=0.83\n",
      "Setting criterion=entropy, splitter=best, min_samples_split=7, train_acc=0.97, val_acc=0.83\n",
      "Setting criterion=entropy, splitter=best, min_samples_split=8, train_acc=0.97, val_acc=0.83\n",
      "Setting criterion=entropy, splitter=best, min_samples_split=9, train_acc=0.96, val_acc=0.83\n",
      "Setting criterion=entropy, splitter=best, min_samples_split=10, train_acc=0.96, val_acc=0.83\n",
      "Setting criterion=entropy, splitter=best, min_samples_split=11, train_acc=0.95, val_acc=0.82\n",
      "Setting criterion=entropy, splitter=best, min_samples_split=12, train_acc=0.95, val_acc=0.82\n",
      "Setting criterion=entropy, splitter=best, min_samples_split=13, train_acc=0.95, val_acc=0.82\n",
      "Setting criterion=entropy, splitter=best, min_samples_split=14, train_acc=0.94, val_acc=0.82\n",
      "Setting criterion=entropy, splitter=best, min_samples_split=15, train_acc=0.94, val_acc=0.82\n",
      "Setting criterion=entropy, splitter=best, min_samples_split=16, train_acc=0.93, val_acc=0.82\n",
      "Setting criterion=entropy, splitter=best, min_samples_split=17, train_acc=0.93, val_acc=0.82\n",
      "Setting criterion=entropy, splitter=best, min_samples_split=18, train_acc=0.93, val_acc=0.82\n",
      "Setting criterion=entropy, splitter=best, min_samples_split=19, train_acc=0.92, val_acc=0.82\n",
      "Setting criterion=entropy, splitter=random, min_samples_split=2, train_acc=1.00, val_acc=0.82\n",
      "Setting criterion=entropy, splitter=random, min_samples_split=3, train_acc=0.99, val_acc=0.82\n",
      "Setting criterion=entropy, splitter=random, min_samples_split=4, train_acc=0.98, val_acc=0.82\n",
      "Setting criterion=entropy, splitter=random, min_samples_split=5, train_acc=0.97, val_acc=0.82\n",
      "Setting criterion=entropy, splitter=random, min_samples_split=6, train_acc=0.96, val_acc=0.82\n",
      "Setting criterion=entropy, splitter=random, min_samples_split=7, train_acc=0.95, val_acc=0.83\n",
      "Setting criterion=entropy, splitter=random, min_samples_split=8, train_acc=0.94, val_acc=0.82\n",
      "Setting criterion=entropy, splitter=random, min_samples_split=9, train_acc=0.93, val_acc=0.82\n",
      "Setting criterion=entropy, splitter=random, min_samples_split=10, train_acc=0.93, val_acc=0.82\n",
      "Setting criterion=entropy, splitter=random, min_samples_split=11, train_acc=0.92, val_acc=0.82\n",
      "Setting criterion=entropy, splitter=random, min_samples_split=12, train_acc=0.91, val_acc=0.83\n",
      "Setting criterion=entropy, splitter=random, min_samples_split=13, train_acc=0.91, val_acc=0.82\n",
      "Setting criterion=entropy, splitter=random, min_samples_split=14, train_acc=0.90, val_acc=0.81\n",
      "Setting criterion=entropy, splitter=random, min_samples_split=15, train_acc=0.90, val_acc=0.81\n",
      "Setting criterion=entropy, splitter=random, min_samples_split=16, train_acc=0.89, val_acc=0.81\n",
      "Setting criterion=entropy, splitter=random, min_samples_split=17, train_acc=0.89, val_acc=0.82\n",
      "Setting criterion=entropy, splitter=random, min_samples_split=18, train_acc=0.89, val_acc=0.82\n",
      "Setting criterion=entropy, splitter=random, min_samples_split=19, train_acc=0.88, val_acc=0.81\n",
      "Best Setting criterion=gini, splitter=best, min_samples_split=3, val_acc=0.84\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# training and hyper-parameter tuning\n",
    "criterions = [\"gini\",\"entropy\"]\n",
    "splitters = ['best', 'random']\n",
    "min_samples_split = np.array(range(2, 20))\n",
    "print(min_samples_split)\n",
    "\n",
    "\n",
    "# trainx\n",
    "# trainy\n",
    "# valx \n",
    "# valy \n",
    "# testx \n",
    "# testy \n",
    "\n",
    "best_min_samples_split = 0\n",
    "best_acc = 0 \n",
    "\n",
    "for criterion in criterions:\n",
    "    for splitter in splitters:\n",
    "        for i in min_samples_split:\n",
    "            clf = DecisionTreeClassifier(criterion = criterion, splitter= splitter, min_samples_split = i )\n",
    "            clf = clf.fit(trainx, trainy)\n",
    "\n",
    "            \n",
    "            # evaluate on trainx, trainy and valx, valy\n",
    "            y_pred_on_train = clf.predict(trainx)\n",
    "            train_acc = sklearn.metrics.accuracy_score(y_true=trainy, y_pred=y_pred_on_train)\n",
    "            \n",
    "            y_pred_on_val = clf.predict(valx)\n",
    "            val_acc = sklearn.metrics.accuracy_score(y_true=valy, y_pred=y_pred_on_val)\n",
    "            \n",
    "            print(\"Setting criterion={}, splitter={}, min_samples_split={}, train_acc={:.2f}, val_acc={:.2f}\".format(criterion, splitter, i, train_acc, val_acc))\n",
    "            if val_acc > best_acc:\n",
    "                best_acc = val_acc\n",
    "                best_min_samples_split = i\n",
    "                best_criterion = criterion\n",
    "                best_splitter = splitter\n",
    "\n",
    "        \n",
    "print(\"Best Setting criterion={}, splitter={}, min_samples_split={}, val_acc={:.2f}\".format(best_criterion, best_splitter, best_min_samples_split, best_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting criterion=gini, splitter=best, min_samples_split=4, test_acc=0.83\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "X_train_val_merge = np.vstack([trainx, valx]) \n",
    "y_train_val_merge = np.hstack([trainy, valy])\n",
    "\n",
    "clf = DecisionTreeClassifier(criterion = best_criterion, splitter= best_splitter, min_samples_split = best_min_samples_split )\n",
    "clf = clf.fit(X_train_val_merge, y_train_val_merge)\n",
    "\n",
    "\n",
    "# evaluate on testx, testy\n",
    "y_pred_on_test = clf.predict(testx)\n",
    "test_acc = sklearn.metrics.accuracy_score(y_true=testy, y_pred=y_pred_on_test)\n",
    "\n",
    "\n",
    "print(\"Setting criterion={}, splitter={}, min_samples_split={}, test_acc={:.2f}\".format(best_criterion, best_splitter, best_min_samples_split, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting criterion=gini, splitter=best, min_samples_split=4, n_estimators=10, train_acc=0.99, val_acc=0.88\n",
      "Setting criterion=gini, splitter=best, min_samples_split=4, n_estimators=15, train_acc=0.99, val_acc=0.89\n",
      "Setting criterion=gini, splitter=best, min_samples_split=4, n_estimators=20, train_acc=1.00, val_acc=0.89\n",
      "Setting criterion=gini, splitter=best, min_samples_split=4, n_estimators=25, train_acc=1.00, val_acc=0.89\n",
      "Setting criterion=gini, splitter=best, min_samples_split=4, n_estimators=30, train_acc=1.00, val_acc=0.89\n",
      "Setting criterion=gini, splitter=best, min_samples_split=4, n_estimators=35, train_acc=1.00, val_acc=0.90\n",
      "Setting criterion=gini, splitter=best, min_samples_split=4, n_estimators=40, train_acc=1.00, val_acc=0.90\n",
      "Setting criterion=gini, splitter=best, min_samples_split=4, n_estimators=45, train_acc=1.00, val_acc=0.90\n",
      "Best Setting criterion=gini, splitter=best, min_samples_split=4, n_estimators=45, val_acc=0.90\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "# training and hyper-parameter tuning\n",
    "\n",
    "n_estimators = np.array(range(10, 50, 5))\n",
    "\n",
    "# trainx\n",
    "# trainy\n",
    "# valx\n",
    "# valy\n",
    "# testx\n",
    "# testy\n",
    "\n",
    "\n",
    "for n in n_estimators:\n",
    "    bagging = BaggingClassifier(DecisionTreeClassifier(criterion=best_criterion, splitter=best_splitter, min_samples_split=best_min_samples_split),\n",
    "                                n_estimators=n)\n",
    "    bagging = bagging.fit(trainx, trainy)\n",
    "\n",
    "    # evaluate on trainx, trainy and valx, valy\n",
    "    y_pred_on_train = bagging.predict(trainx)\n",
    "    train_acc = sklearn.metrics.accuracy_score(\n",
    "        y_true=trainy, y_pred=y_pred_on_train)\n",
    "\n",
    "    y_pred_on_val = bagging.predict(valx)\n",
    "    val_acc = sklearn.metrics.accuracy_score(y_true=valy, y_pred=y_pred_on_val)\n",
    "\n",
    "    print(\"Setting criterion={}, splitter={}, min_samples_split={}, n_estimators={}, train_acc={:.2f}, val_acc={:.2f}\".format(\n",
    "        best_criterion, best_splitter, best_min_samples_split, n, train_acc, val_acc))\n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        best_n_estimators = n\n",
    "\n",
    "\n",
    "print(\"Best Setting criterion={}, splitter={}, min_samples_split={}, n_estimators={}, val_acc={:.2f}\".format(\n",
    "    best_criterion, best_splitter, best_min_samples_split, best_n_estimators, best_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting criterion=gini, splitter=best, min_samples_split=4, test_acc=0.89\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "X_train_val_merge = np.vstack([trainx, valx]) \n",
    "y_train_val_merge = np.hstack([trainy, valy])\n",
    "\n",
    "bagging = BaggingClassifier(DecisionTreeClassifier(criterion = best_criterion, splitter= best_splitter, min_samples_split = best_min_samples_split ),\n",
    "                                n_estimators=best_n_estimators)\n",
    "bagging = bagging.fit(X_train_val_merge, y_train_val_merge)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# evaluate on testx, testy\n",
    "y_pred_on_test = bagging.predict(testx)\n",
    "test_acc = sklearn.metrics.accuracy_score(y_true=testy, y_pred=y_pred_on_test)\n",
    "\n",
    "\n",
    "print(\"Setting criterion={}, splitter={}, min_samples_split={}, test_acc={:.2f}\".format(best_criterion, best_splitter, best_min_samples_split, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting criterion=gini, splitter=best, min_samples_split=4, n_estimators=10, train_acc=1.00, val_acc=0.88\n",
      "Setting criterion=gini, splitter=best, min_samples_split=4, n_estimators=15, train_acc=1.00, val_acc=0.89\n",
      "Setting criterion=gini, splitter=best, min_samples_split=4, n_estimators=20, train_acc=1.00, val_acc=0.89\n",
      "Setting criterion=gini, splitter=best, min_samples_split=4, n_estimators=25, train_acc=1.00, val_acc=0.89\n",
      "Setting criterion=gini, splitter=best, min_samples_split=4, n_estimators=30, train_acc=1.00, val_acc=0.89\n",
      "Setting criterion=gini, splitter=best, min_samples_split=4, n_estimators=35, train_acc=1.00, val_acc=0.89\n",
      "Setting criterion=gini, splitter=best, min_samples_split=4, n_estimators=40, train_acc=1.00, val_acc=0.89\n",
      "Setting criterion=gini, splitter=best, min_samples_split=4, n_estimators=45, train_acc=1.00, val_acc=0.89\n",
      "Best Setting criterion=gini, splitter=best, min_samples_split=4, n_estimators=45, val_acc=0.90\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "# training and hyper-parameter tuning\n",
    "\n",
    "n_estimators = np.array(range(10, 50, 5))\n",
    "\n",
    "# trainx\n",
    "# trainy\n",
    "# valx\n",
    "# valy\n",
    "# testx\n",
    "# testy\n",
    "\n",
    "\n",
    "for n in n_estimators:\n",
    "    ada_boost = AdaBoostClassifier(DecisionTreeClassifier(criterion=best_criterion, splitter=best_splitter, min_samples_split=best_min_samples_split),\n",
    "                                n_estimators=n)\n",
    "    ada_boost = ada_boost.fit(trainx, trainy)\n",
    "\n",
    "    # evaluate on trainx, trainy and valx, valy\n",
    "    y_pred_on_train = ada_boost.predict(trainx)\n",
    "    train_acc = sklearn.metrics.accuracy_score(\n",
    "        y_true=trainy, y_pred=y_pred_on_train)\n",
    "\n",
    "    y_pred_on_val = ada_boost.predict(valx)\n",
    "    val_acc = sklearn.metrics.accuracy_score(y_true=valy, y_pred=y_pred_on_val)\n",
    "\n",
    "    print(\"Setting criterion={}, splitter={}, min_samples_split={}, n_estimators={}, train_acc={:.2f}, val_acc={:.2f}\".format(\n",
    "        best_criterion, best_splitter, best_min_samples_split, n, train_acc, val_acc))\n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        best_n_estimators = n\n",
    "\n",
    "\n",
    "print(\"Best Setting criterion={}, splitter={}, min_samples_split={}, n_estimators={}, val_acc={:.2f}\".format(\n",
    "    best_criterion, best_splitter, best_min_samples_split, best_n_estimators, best_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting criterion=gini, splitter=best, min_samples_split=4, test_acc=0.89\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "X_train_val_merge = np.vstack([trainx, valx]) \n",
    "y_train_val_merge = np.hstack([trainy, valy])\n",
    "\n",
    "ada_boost = AdaBoostClassifier(DecisionTreeClassifier(criterion = best_criterion, splitter= best_splitter, min_samples_split = best_min_samples_split ),\n",
    "                                n_estimators=best_n_estimators)\n",
    "ada_boost = ada_boost.fit(X_train_val_merge, y_train_val_merge)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# evaluate on testx, testy\n",
    "y_pred_on_test = ada_boost.predict(testx)\n",
    "test_acc = sklearn.metrics.accuracy_score(y_true=testy, y_pred=y_pred_on_test)\n",
    "\n",
    "\n",
    "print(\"Setting criterion={}, splitter={}, min_samples_split={}, test_acc={:.2f}\".format(best_criterion, best_splitter, best_min_samples_split, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting criterion=gini, min_samples_split=2, n_estimators=30, train_acc=1.00, val_acc=0.88\n",
      "Setting criterion=gini, min_samples_split=2, n_estimators=35, train_acc=1.00, val_acc=0.88\n",
      "Setting criterion=gini, min_samples_split=2, n_estimators=40, train_acc=1.00, val_acc=0.88\n",
      "Setting criterion=gini, min_samples_split=2, n_estimators=45, train_acc=1.00, val_acc=0.88\n",
      "Setting criterion=gini, min_samples_split=6, n_estimators=30, train_acc=0.98, val_acc=0.87\n",
      "Setting criterion=gini, min_samples_split=6, n_estimators=35, train_acc=0.99, val_acc=0.87\n",
      "Setting criterion=gini, min_samples_split=6, n_estimators=40, train_acc=0.99, val_acc=0.87\n",
      "Setting criterion=gini, min_samples_split=6, n_estimators=45, train_acc=0.99, val_acc=0.87\n",
      "Setting criterion=gini, min_samples_split=10, n_estimators=30, train_acc=0.96, val_acc=0.87\n",
      "Setting criterion=gini, min_samples_split=10, n_estimators=35, train_acc=0.96, val_acc=0.87\n",
      "Setting criterion=gini, min_samples_split=10, n_estimators=40, train_acc=0.97, val_acc=0.87\n",
      "Setting criterion=gini, min_samples_split=10, n_estimators=45, train_acc=0.97, val_acc=0.86\n",
      "Setting criterion=gini, min_samples_split=14, n_estimators=30, train_acc=0.94, val_acc=0.86\n",
      "Setting criterion=gini, min_samples_split=14, n_estimators=35, train_acc=0.95, val_acc=0.86\n",
      "Setting criterion=gini, min_samples_split=14, n_estimators=40, train_acc=0.95, val_acc=0.86\n",
      "Setting criterion=gini, min_samples_split=14, n_estimators=45, train_acc=0.95, val_acc=0.86\n",
      "Setting criterion=gini, min_samples_split=18, n_estimators=30, train_acc=0.93, val_acc=0.85\n",
      "Setting criterion=gini, min_samples_split=18, n_estimators=35, train_acc=0.93, val_acc=0.85\n",
      "Setting criterion=gini, min_samples_split=18, n_estimators=40, train_acc=0.93, val_acc=0.85\n",
      "Setting criterion=gini, min_samples_split=18, n_estimators=45, train_acc=0.93, val_acc=0.85\n",
      "Setting criterion=entropy, min_samples_split=2, n_estimators=30, train_acc=1.00, val_acc=0.88\n",
      "Setting criterion=entropy, min_samples_split=2, n_estimators=35, train_acc=1.00, val_acc=0.88\n",
      "Setting criterion=entropy, min_samples_split=2, n_estimators=40, train_acc=1.00, val_acc=0.88\n",
      "Setting criterion=entropy, min_samples_split=2, n_estimators=45, train_acc=1.00, val_acc=0.88\n",
      "Setting criterion=entropy, min_samples_split=6, n_estimators=30, train_acc=0.99, val_acc=0.88\n",
      "Setting criterion=entropy, min_samples_split=6, n_estimators=35, train_acc=0.99, val_acc=0.88\n",
      "Setting criterion=entropy, min_samples_split=6, n_estimators=40, train_acc=0.99, val_acc=0.88\n",
      "Setting criterion=entropy, min_samples_split=6, n_estimators=45, train_acc=0.99, val_acc=0.88\n",
      "Setting criterion=entropy, min_samples_split=10, n_estimators=30, train_acc=0.97, val_acc=0.86\n",
      "Setting criterion=entropy, min_samples_split=10, n_estimators=35, train_acc=0.97, val_acc=0.87\n",
      "Setting criterion=entropy, min_samples_split=10, n_estimators=40, train_acc=0.97, val_acc=0.87\n",
      "Setting criterion=entropy, min_samples_split=10, n_estimators=45, train_acc=0.97, val_acc=0.87\n",
      "Setting criterion=entropy, min_samples_split=14, n_estimators=30, train_acc=0.95, val_acc=0.86\n",
      "Setting criterion=entropy, min_samples_split=14, n_estimators=35, train_acc=0.95, val_acc=0.86\n",
      "Setting criterion=entropy, min_samples_split=14, n_estimators=40, train_acc=0.95, val_acc=0.86\n",
      "Setting criterion=entropy, min_samples_split=14, n_estimators=45, train_acc=0.95, val_acc=0.86\n",
      "Setting criterion=entropy, min_samples_split=18, n_estimators=30, train_acc=0.94, val_acc=0.86\n",
      "Setting criterion=entropy, min_samples_split=18, n_estimators=35, train_acc=0.94, val_acc=0.85\n",
      "Setting criterion=entropy, min_samples_split=18, n_estimators=40, train_acc=0.94, val_acc=0.86\n",
      "Setting criterion=entropy, min_samples_split=18, n_estimators=45, train_acc=0.94, val_acc=0.86\n",
      "Best Setting criterion=entropy, min_samples_split=2, n_estimators=40, val_acc=0.88\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# training and hyper-parameter tuning\n",
    "criterions = [\"gini\", \"entropy\"]\n",
    "splitters = ['best', 'random']\n",
    "min_samples_split = np.array(range(2, 20, 4))\n",
    "n_estimators = np.array(range(30, 50, 5))\n",
    "\n",
    "\n",
    "# trainx\n",
    "# trainy\n",
    "# valx\n",
    "# valy\n",
    "# testx\n",
    "# testy\n",
    "\n",
    "best_min_samples_split = 0\n",
    "best_acc = 0\n",
    "best_n_estimators = 0\n",
    "\n",
    "for criterion in criterions:\n",
    "    for i in min_samples_split:\n",
    "        for n in n_estimators:\n",
    "            rf = RandomForestClassifier(\n",
    "                criterion=criterion, min_samples_split=i, n_estimators=n)\n",
    "            rf = rf.fit(trainx, trainy)\n",
    "\n",
    "            # evaluate on trainx, trainy and valx, valy\n",
    "            y_pred_on_train = rf.predict(trainx)\n",
    "            train_acc = sklearn.metrics.accuracy_score(\n",
    "                y_true=trainy, y_pred=y_pred_on_train)\n",
    "\n",
    "            y_pred_on_val = rf.predict(valx)\n",
    "            val_acc = sklearn.metrics.accuracy_score(\n",
    "                y_true=valy, y_pred=y_pred_on_val)\n",
    "\n",
    "            print(\"Setting criterion={}, min_samples_split={}, n_estimators={}, train_acc={:.2f}, val_acc={:.2f}\".format(\n",
    "                criterion, i, n, train_acc, val_acc))\n",
    "            if val_acc > best_acc:\n",
    "                best_acc = val_acc\n",
    "                best_min_samples_split = i\n",
    "                best_criterion = criterion\n",
    "                best_n_estimators = n\n",
    "                \n",
    "\n",
    "\n",
    "print(\"Best Setting criterion={}, min_samples_split={}, n_estimators={}, val_acc={:.2f}\".format(\n",
    "    best_criterion, best_min_samples_split,best_n_estimators, best_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting criterion=gini, splitter=best, min_samples_split=4, test_acc=0.88\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "X_train_val_merge = np.vstack([trainx, valx]) \n",
    "y_train_val_merge = np.hstack([trainy, valy])\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "                criterion=criterion, min_samples_split=i, n_estimators=best_n_estimators)\n",
    "rf = rf.fit(X_train_val_merge, y_train_val_merge)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# evaluate on testx, testy\n",
    "y_pred_on_test = rf.predict(testx)\n",
    "test_acc = sklearn.metrics.accuracy_score(y_true=testy, y_pred=y_pred_on_test)\n",
    "\n",
    "\n",
    "print(\"Setting criterion={}, min_samples_split={}, n_estimators={}, test_acc={:.2f}\".format(best_criterion, best_min_samples_split,best_n_estimators, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions:\n",
    "\n",
    "Please report in you submission the following for each classifier:\n",
    "1. The best result on the validaton set\n",
    "2. Hyperparameter values for the classifier\n",
    "3. The result on the test set.\n",
    "\n",
    "Apart from the above, please provide your comments and observations on the results of the different classifiers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ansewers: \n",
    "1. Decesion Tree:\n",
    "    - The best result in validation is 0.84\n",
    "    - The hyperparameters are criterion=gini, splitter=best, min_samples_split=3\n",
    "    - The best result in test-set is 0.83\n",
    "\n",
    "2. Bagging:\n",
    "    - The best result in validation is 0.90 \n",
    "    - The hyperparameters are criterion=gini, splitter=best, min_samples_split=4, n_estimators=45\n",
    "    - The best result in test-set is 0.89\n",
    "\n",
    "2. AdaBoost:\n",
    "    - The best result in validation is 0.90\n",
    "    - The hyperparameters are criterion=gini, splitter=best, min_samples_split=4, n_estimators=45\n",
    "    - The best result in test-set is 0.89\n",
    "\n",
    "2. Random Forest:\n",
    "    - The best result in validation is 0.88\n",
    "    - The hyperparameters are criterion=entropy, min_samples_split=2, n_estimators=40\n",
    "    - The best result in test-set is 0.88\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXTRA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the Statlog (Vehicle Silhouettes) Data Set (https://archive.ics.uci.edu/ml/datasets/Statlog+%28Vehicle+Silhouettes%29)\n",
    "\n",
    "It has 18 features and following four classes:\n",
    "**OPEL, SAAB, BUS, VAN**\n",
    "\n",
    "The purpose is to classify a given silhouette as one of four types of vehicle, using a set of features extracted from the silhouette. The vehicle may be viewed from one of many different angles. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Decision Tree classifer to train the classifier with train/val/test partition as 70/15/15 (random seed=777)\n",
    "Fine-tune the tree on the validation set.\n",
    "\n",
    "1. Report the performance on the test set\n",
    "2. Extract the decision rules for classifiecation.\n",
    "3. Plot the decision tree as an image using some library.\n",
    "\n",
    "**Note:** You have library support in Python for visualizing the learnt decision trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
