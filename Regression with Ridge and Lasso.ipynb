{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression with Ridge and Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the assignment, you need the predict the price of the house (i.e., column 4 of the csv file) and the features provided to you are 'len', 'width', 'rooms' (i.e., the first 3 columns of the csv file). You can use the sklearn library to use ``LinearRegression``, ``Ridge``, and ``Lasso`` from ``sklearn.linear_model``. Moreover, if you feel the need to expand the features to polynomials (say degree 2) you can either transform the CSV file manually or use the ``PolynomialFeatures`` from ``sklearn.preprocessing``. You might realize that adding polynomial features can improve the results but you have to be careful about overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard includes\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "# Routines for linear regression\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "# Set label size for plots\n",
    "matplotlib.rc('xtick', labelsize=14) \n",
    "matplotlib.rc('ytick', labelsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.genfromtxt('LandPriceTrain.csv', delimiter=',')\n",
    "features = ['len', 'width', 'rooms']\n",
    "x_train = data[:,0:3] # predictors\n",
    "y_train = data[:,3] # response variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.genfromtxt('LandPriceTest.csv', delimiter=',')\n",
    "x_test = data[:,0:3] # predictors\n",
    "y_test = data[:,3] # response variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. What best can we acheive if we have no predictors and only response (House Prices) values in the training data? What will be the mean error?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best result if we have no predictors:\n",
      "Prediction (train): 84779.4500 \n",
      "Mean squared error (train): 3210718511.6475 \n",
      "Mean error (train): 56663.2024 \n",
      "Prediction (test): 87341.6000 \n",
      "Mean squared error (test): 3536459658.6400 \n",
      "Mean error (test): 59468.1399 \n"
     ]
    }
   ],
   "source": [
    "### START CODE HERE ###\n",
    "print ('The best result if we have no predictors:')\n",
    "print (\"Prediction (train): %0.4f \"% np.mean(y_train))\n",
    "print (\"Mean squared error (train): %0.4f \"% np.var(y_train))\n",
    "print (\"Mean error (train): %0.4f \"% np.std(y_train))\n",
    "print (\"Prediction (test): %0.4f \"% np.mean(y_test))\n",
    "print (\"Mean squared error (test): %0.4f \"% np.var(y_test))\n",
    "print (\"Mean error (test): %0.4f \"% np.std(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Let's now use the features and see what we can observe  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_subset_regression(x,y,flist):\n",
    "    if len(flist) < 1:\n",
    "        print (\"Need at least one feature\")\n",
    "        return\n",
    "    for f in flist:\n",
    "        if (f < 0) or (f > 2):\n",
    "            print (\"Feature index is out of bounds\")\n",
    "            return\n",
    "    ### COMPLETE CODE BELOW by creating an instance of LinearRegression\n",
    "    regr = linear_model.LinearRegression()\n",
    "    regr.fit(x[:,flist], y)\n",
    "    return regr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w =  [ 3010.83212779  2914.47821951 -2420.72225879]\n",
      "b =  -77044.07528278603\n",
      "Mean squared error (train):  217514383.15439194\n",
      "Mean error (train):  12629.08643427137\n",
      "Mean squared error (test):  158706743.7864198\n",
      "Mean error (test):  9999.972138004141\n"
     ]
    }
   ],
   "source": [
    "flist = [0,1,2]\n",
    "regr = feature_subset_regression(x_train,y_train,flist)\n",
    "\n",
    "print (\"w = \", regr.coef_)\n",
    "print (\"b = \", regr.intercept_)\n",
    "\n",
    "print (\"Mean squared error (train): \", mean_squared_error(y_train, regr.predict(x_train[:,flist])))\n",
    "print (\"Mean error (train): \", mean_absolute_error(y_train, regr.predict(x_train[:,flist])))\n",
    "print (\"Mean squared error (test): \", mean_squared_error(y_test, regr.predict(x_test[:,flist])))\n",
    "print (\"Mean error (test): \", mean_absolute_error(y_test, regr.predict(x_test[:,flist])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. It seems we are underfitting as the train and test error are significantly high. Let's try to use polynomial features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try incorporating polynomial features (say of degree 2) and see how you perform on the train and the test set. You can either transform the CSV file manually or use the ``PolynomialFeatures`` from ``sklearn.preprocessing``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "#try to expand the fetaures fit the linear regression and report the results\n",
    "\n",
    "def PolynomialFeatures_subset_regression(x,y,flist):\n",
    "    if len(flist) < 1:\n",
    "        print (\"Need at least one feature\")\n",
    "        return\n",
    "    for f in flist:\n",
    "        if (f < 0) or (f > 9):\n",
    "            print (\"Feature index is out of bounds\")\n",
    "            return\n",
    "    ### COMPLETE CODE BELOW by creating an instance of LinearRegression\n",
    "    regr = linear_model.LinearRegression()\n",
    "    regr.fit(x[:,flist], y)\n",
    "    return regr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20, 9), (10, 9))"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flist = [0,1,2]\n",
    "poly = PolynomialFeatures(2, include_bias=False)\n",
    "x_train = poly.fit_transform(x_train[:,flist])\n",
    "x_test = poly.fit_transform(x_test[:,flist])\n",
    "\n",
    "x_train.shape, x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w =  [ 0.00000000e+00 -7.95807864e-13  9.28826930e+02  0.00000000e+00\n",
      " -7.09407348e-44  9.28826930e+02 -1.99136489e-59  9.28826930e+02\n",
      "  1.37837894e+01]\n",
      "b =  -1413.5787105344498\n",
      "Mean squared error (train):  1179955104.4133008\n",
      "Mean error (train):  28386.18577169961\n",
      "Mean squared error (test):  2025065980.4057744\n",
      "Mean error (test):  38135.04041554661\n"
     ]
    }
   ],
   "source": [
    "### UPDATE THE CODE BELOW ###\n",
    "flist = [0,1,2,3,4,5,6,7,8]\n",
    "regr = PolynomialFeatures_subset_regression(x_train, y_train, flist)\n",
    "\n",
    "print (\"w = \", regr.coef_)\n",
    "print (\"b = \", regr.intercept_)\n",
    "\n",
    "print (\"Mean squared error (train): \", mean_squared_error(y_train, regr.predict(x_train[:,flist])))\n",
    "print (\"Mean error (train): \", mean_absolute_error(y_train, regr.predict(x_train[:,flist])))\n",
    "print (\"Mean squared error (test): \", mean_squared_error(y_test, regr.predict(x_test[:,flist])))\n",
    "print (\"Mean error (test): \", mean_absolute_error(y_test, regr.predict(x_test[:,flist])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It seems we are overfitting as the train error is significantly lower than the test error. Let's try some regularization techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_subset_ridge(x,y,flist, alp):\n",
    "    if len(flist) < 1:\n",
    "        print (\"Need at least one feature\")\n",
    "        return\n",
    "    for f in flist:\n",
    "        if (f < 0) or (f > 8):\n",
    "            print (\"Feature index is out of bounds\")\n",
    "            return\n",
    "    ### COMPLETE CODE BELOW by creating an instance of Ridge, be careful of the parameter\n",
    "    regr = Ridge(alpha = alp) \n",
    "    regr.fit(x[:,flist], y)\n",
    "    return regr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w =  [  0.           0.         928.66087025   0.           0.\n",
      " 928.66087026   0.         928.66087026  13.79187221]\n",
      "b =  -1407.2999446951872\n",
      "Mean squared error (train):  1179955105.5696177\n",
      "Mean error (train):  28386.300099264976\n",
      "Mean squared error (test):  2025017860.8386319\n",
      "Mean error (test):  38134.58438110156\n"
     ]
    }
   ],
   "source": [
    "flist = [0,1,2,3,4,5,6,7,8]\n",
    "regr = feature_subset_ridge(x_train,y_train,flist, 0.05)\n",
    "\n",
    "print (\"w = \", regr.coef_)\n",
    "print (\"b = \", regr.intercept_)\n",
    "\n",
    "print (\"Mean squared error (train): \", mean_squared_error(y_train, regr.predict(x_train[:,flist])))\n",
    "print (\"Mean error (train): \", mean_absolute_error(y_train, regr.predict(x_train[:,flist])))\n",
    "print (\"Mean squared error (test): \", mean_squared_error(y_test, regr.predict(x_test[:,flist])))\n",
    "print (\"Mean error (test): \", mean_absolute_error(y_test, regr.predict(x_test[:,flist])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_subset_lasso(x,y,flist, alp):\n",
    "    if len(flist) < 1:\n",
    "        print (\"Need at least one feature\")\n",
    "        return\n",
    "    for f in flist:\n",
    "        if (f < 0) or (f > 8):\n",
    "            print (\"Feature index is out of bounds\")\n",
    "            return\n",
    "    ### COMPLETE CODE BELOW by creating an instance of Lasso, be careful of the parameters  \n",
    "    regr = Lasso(alpha = alp)\n",
    "    regr.fit(x[:,flist], y)\n",
    "    return regr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w =  [0.00000000e+00 0.00000000e+00 2.55181954e+03 0.00000000e+00\n",
      " 0.00000000e+00 3.92972587e-13 0.00000000e+00 7.48519213e-14\n",
      " 1.75891198e+01]\n",
      "b =  1545.6786983396014\n",
      "Mean squared error (train):  1180211667.8754098\n",
      "Mean error (train):  28478.725999645114\n",
      "Mean squared error (test):  2002699444.2636254\n",
      "Mean error (test):  37919.915186784994\n"
     ]
    }
   ],
   "source": [
    "flist = [0,1,2,3,4,5,6,7,8]\n",
    "regr = feature_subset_lasso(x_train,y_train,flist, 1150)\n",
    "print (\"w = \", regr.coef_)\n",
    "print (\"b = \", regr.intercept_)\n",
    "print (\"Mean squared error (train): \", mean_squared_error(y_train, regr.predict(x_train[:,flist])))\n",
    "print (\"Mean error (train): \", mean_absolute_error(y_train, regr.predict(x_train[:,flist])))\n",
    "print (\"Mean squared error (test): \", mean_squared_error(y_test, regr.predict(x_test[:,flist])))\n",
    "print (\"Mean error (test): \", mean_absolute_error(y_test, regr.predict(x_test[:,flist])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document your observation and understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We use polynomial features to fix under fitting \n",
    "* To deal with overfitting we use two types of regularization:\n",
    "    1. Ridge \n",
    "    2. Lasso \n",
    "* Ridge regularization will make the value of the coefficient to tend towards zero\n",
    "* Lasso regularization will select some features (set some coefficients zero)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Questions (Optional-Extra)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Implement the closed-form solution for Ridge\n",
    "2. Implement the iterative solution (Gradient Descent) for Ridge\n",
    "3. Implement the iterative solution for Lasso\n",
    "4. Use the sklearn linear_model.ElasticNet and try on the above problem.\n",
    "\n",
    "Compare your implemented solutions with the built-in solutions on the above problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
